{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Task_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza1122/Deep-Learning-Ass/blob/master/Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wjif_5FsdOFz"
      },
      "source": [
        "<strong><h1>Efficient Training of Convolutional Neural Network </h1></strong>\n",
        "\n",
        "\n",
        "\n",
        "<h2> What to submit </h2>\n",
        "\n",
        "<ol>\n",
        "<li>\tYour Python notebook solution source file. </li>\n",
        "<li>\tThe output of your Python notebook solution <emph>exported</emph> in HTML format.</li>\n",
        "<li>\tExtra files needed to complete your task, if any (e.g., images used in your answers).</li>\n",
        "</ol>\n",
        "</p>\n",
        "\n",
        "\n",
        "<h2> Warning </h2>\n",
        "\n",
        "Some components of this task may involve heavy computation that runs for a long duration. Please start early to avoid missing the due date.\n",
        "\n",
        "<h2> Marking criteria </h2>\n",
        "\n",
        "<p>\n",
        "Your submission will be assessed using the following criteria.\n",
        "\n",
        "<ul>\n",
        "<li> Showing good effort through completed tasks.</li>\n",
        "<li> Applying deep learning theory to design suitable deep learning solutions for the tasks.</li>\n",
        "<li> Critically evaluating and reflecting on the pros and cons of various design decisions.</li>\n",
        "<li> Demonstrating creativity and resourcefulness in providing unique individual solutions.</li>\n",
        "<li> Showing attention to details through a good quality  report.</li>\n",
        "</ul>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "Indicative weights of various tasks are provided, but the task will be assessed by the overall quality per the above criteria.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "twFQbltnm8da"
      },
      "source": [
        "## Task objective\n",
        "\n",
        "You will experience training a much deeper network on a large-scale dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3ITc1hw_o7qV"
      },
      "source": [
        "## Task 1 Solving Fashion-MNIST with Convolutional Neural Networks\n",
        "\n",
        "\n",
        "In previous task, you tackled the image classification problem in Fashion-MNIST. There, you used a Densely Connected Neural Network. You should now know that is not an optimal model architecture for the problem. In this task, you will apply the best practices of deep-learning computer vision to improve the image classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zdHwmgwOpEfx"
      },
      "source": [
        "### Task 1.1 Revisit Fashion-MNIST classification with DNN\n",
        "\n",
        "\n",
        "Review your previous tasks solution, and reproduce the experiment here. Try to improve the model without changing the model architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXCFMPkGis13",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaCzaqzc9dYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "dfcfebaf-3cc4-478f-a98c-cab8e61ea9b9"
      },
      "source": [
        "# Loading the mnist dataset with Keras\n",
        "#importing libraries\n",
        "from keras import models, layers\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import time\n",
        "import keras\n",
        "tfds.disable_progress_bar()\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'fashion_mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    with_info=True,\n",
        ")\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']                  \n",
        "\n",
        "\n",
        "# preprocessing using pipelining\n",
        "def preprocess(features):\n",
        "    image = features['image']\n",
        "    image = tf.image.resize(image, (28,28))\n",
        "    image = image / 255.0\n",
        "    return image, features['label']  \n",
        "\n",
        "\n",
        "#creating input pipeline\n",
        "ds_train = ds_train.map(\n",
        "    preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "#ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(128)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)   \n",
        "\n",
        "\n",
        "\n",
        "#creating pipeling for test images\n",
        "ds_test = ds_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.batch(128)\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "import datetime\n",
        "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
        "path=\"logs/fit/\" +datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "callback=tf.keras.callbacks.TensorBoard(log_dir=path,histogram_freq=1,write_graph=True)\n",
        "\n",
        "\n",
        "#setting up a model for training\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Dense(128,activation='relu'),\n",
        "  tf.keras.layers.Dense(128,activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "model.summary()\n",
        "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset fashion_mnist/3.0.0 (download: 29.45 MiB, generated: Unknown size, total: 29.45 MiB) to /root/tensorflow_datasets/fashion_mnist/3.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/fashion_mnist/3.0.0.incompleteE4GWKZ/fashion_mnist-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/fashion_mnist/3.0.0.incompleteE4GWKZ/fashion_mnist-test.tfrecord\n",
            "\u001b[1mDataset fashion_mnist downloaded and prepared to /root/tensorflow_datasets/fashion_mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 118,282\n",
            "Trainable params: 118,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpUQ0Yh69_Am",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "615ae996-7ca0-4ecc-abc8-aff4e78f0a19"
      },
      "source": [
        "# We are using Gradient Norm With Adam\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,clipnorm=1.0),metrics=['accuracy'])\n",
        "history=model.fit(ds_train,epochs=1,validation_data=ds_test,callbacks=[callback])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r      1/Unknown - 0s 775us/step - loss: 2.3988 - accuracy: 0.0938WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r      2/Unknown - 0s 48ms/step - loss: 2.2870 - accuracy: 0.1445 WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0869s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0869s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5341 - accuracy: 0.8140 - val_loss: 0.4424 - val_accuracy: 0.8412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cfTL_5kPsl4j"
      },
      "source": [
        "### Task 1.2 Train a ConvNet from scratch\n",
        "\n",
        "\n",
        "Build a ConvNet to replace the densely connected network in Task 1.1. Report the classification accuracy on the test set. Aim to achieve higher accuracy. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHcl6HHi6yVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now Using CNN\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMkifrtN7vPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "5f041936-f789-4712-bd58-8952f4e7de68"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "model.summary()\n",
        "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA2291387zeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "94693f7c-c396-4324-e81b-98400a2d2b82"
      },
      "source": [
        "# We are using Gradient Norm With Adam\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,clipnorm=1.0),metrics=['accuracy'])\n",
        "history=model.fit(ds_train,epochs=10,validation_data=ds_test,callbacks=[callback])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "      2/Unknown - 0s 38ms/step - loss: 2.2697 - accuracy: 0.1484 WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0655s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0655s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5230 - accuracy: 0.8123 - val_loss: 0.3806 - val_accuracy: 0.8616\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3352 - accuracy: 0.8805 - val_loss: 0.3327 - val_accuracy: 0.8803\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2892 - accuracy: 0.8938 - val_loss: 0.3045 - val_accuracy: 0.8906\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2586 - accuracy: 0.9054 - val_loss: 0.2787 - val_accuracy: 0.8980\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2339 - accuracy: 0.9143 - val_loss: 0.2700 - val_accuracy: 0.9014\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2128 - accuracy: 0.9217 - val_loss: 0.2590 - val_accuracy: 0.9042\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1977 - accuracy: 0.9271 - val_loss: 0.2472 - val_accuracy: 0.9122\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1798 - accuracy: 0.9330 - val_loss: 0.2550 - val_accuracy: 0.9075\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1657 - accuracy: 0.9384 - val_loss: 0.2490 - val_accuracy: 0.9117\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1523 - accuracy: 0.9436 - val_loss: 0.2495 - val_accuracy: 0.9140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ge04wD71nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "f2905fc8-1a8b-4f58-ae55-988b20f31970"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qbhr44DnMbKZ"
      },
      "source": [
        "\n",
        "### Task 1.3 Build an input pipeline for data augmentation\n",
        "\n",
        "\n",
        "Build a data preprocessing pipeline to perform data augmentation. (You may use Keras ImageDataGenerator or write your own transformations.)\n",
        "\n",
        "- Report the new classification accuracy. Make sure that you use the same number of training epochs as in Task 1.2.\n",
        "\n",
        "- Profile your input pipeline to identify the most time-consuming operation. What actions have you taken to address that slow operation? (*Hint: You may use the [TensorFlow Profiler](https://github.com/tensorflow/profiler).*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gymI-3NDAQwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "81d9c82a-920b-40ef-ec3b-7d252e3eaae3"
      },
      "source": [
        "#Data Augmentation\n",
        "IMG_SIZE = 180\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.experimental.preprocessing.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "\n",
        "#Now Using CNN\n",
        "model = tf.keras.models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    data_augmentation,                                \n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-9ed6db843e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m resize_and_rescale = tf.keras.Sequential([\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRescaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, height, width, interpolation, name, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpolation_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_interpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResizing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\u001b[0m in \u001b[0;36mget_interpolation\u001b[0;34m(interpolation)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_interpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m   \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_RESIZE_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m     raise NotImplementedError(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95HlzWcxBfL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "6fa2f7d8-0245-459c-ab58-01416b09c7be"
      },
      "source": [
        "# We are using Gradient Norm With Adam\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001,clipnorm=1.0),metrics=['accuracy'])\n",
        "history=model.fit(ds_train,epochs=10,validation_data=ds_test,callbacks=[callback])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "      2/Unknown - 0s 61ms/step - loss: 2.3024 - accuracy: 0.1133 WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0337s vs `on_train_batch_end` time: 0.0866s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0337s vs `on_train_batch_end` time: 0.0866s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 39s 84ms/step - loss: 1.6143 - accuracy: 0.4075 - val_loss: 1.0902 - val_accuracy: 0.6062\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 1.1242 - accuracy: 0.5896 - val_loss: 0.9705 - val_accuracy: 0.6509\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 1.0026 - accuracy: 0.6334 - val_loss: 0.8638 - val_accuracy: 0.6746\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.9521 - accuracy: 0.6466 - val_loss: 0.8424 - val_accuracy: 0.6860\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.9136 - accuracy: 0.6628 - val_loss: 0.8501 - val_accuracy: 0.6907\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.8910 - accuracy: 0.6729 - val_loss: 0.7695 - val_accuracy: 0.7175\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.8685 - accuracy: 0.6805 - val_loss: 0.8182 - val_accuracy: 0.6923\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.8500 - accuracy: 0.6901 - val_loss: 0.7619 - val_accuracy: 0.7213\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.8340 - accuracy: 0.6971 - val_loss: 0.7346 - val_accuracy: 0.7320\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.8176 - accuracy: 0.7020 - val_loss: 0.7644 - val_accuracy: 0.7139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6LL_q69E8DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "prctXU4BswKK"
      },
      "source": [
        "### Task 1.4 Fashion-MNIST with transfer learning\n",
        "\n",
        "\n",
        "\n",
        "Use a pretrained model as the convolutional base to improve the classification performance. (Hint: You may use models in Keras Applications or those in the TensorFlow Hub.)\n",
        "\n",
        "- Try both with fine-tuning and without fine-tuning.\n",
        "- Report the model performance as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAWxzCWWO7rQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
        "X_train = np.dstack([trainX] * 3)\n",
        "X_test = np.dstack([testX]*3)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqXciOcAjAbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "48619de5-57d5-4343-cb76-b32b1bbc6a29"
      },
      "source": [
        "X_train = X_train.reshape(-1, 28,28,3)\n",
        "X_test= X_test.reshape (-1,28,28,3)\n",
        "\n",
        "# Display the new shape\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 3), (10000, 28, 28, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcl5H_92mLAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = X_train / 255.\n",
        "test_X = X_train / 255.\n",
        "train_X = train_X.astype('float32')\n",
        "test_X = test_X.astype('float32')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmwmXu3znlmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f7c3dfa6-d051-4f20-9abc-229d9f0350cb"
      },
      "source": [
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "train_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in train_X])\n",
        "test_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in test_X])\n",
        "#train_x = preprocess_input(x)\n",
        "train_X.shape, test_X.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 48, 48, 3), (60000, 48, 48, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkmUPY45mXIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalise the data and change data type\n",
        "train_X = train_X / 255.\n",
        "test_X = test_X / 255.\n",
        "train_X = train_X.astype('float32')\n",
        "test_X = test_X.astype('float32')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqPQymjOnThS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_Y_one_hot = to_categorical(trainY)\n",
        "test_Y_one_hot = to_categorical(testY)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt4BGJtVRGsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X,valid_X,train_label,valid_label = train_test_split(train_X,\n",
        "                                                           train_Y_one_hot,\n",
        "                                                           test_size=0.2,\n",
        "                                                           random_state=13\n",
        "                                                           )"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-9VGaFZ33Qx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "33a1d192-9bc8-4b25-9d69-1e33538afacc"
      },
      "source": [
        "# Finally check the data size whether it is as per tensorflow and VGG16 requirement\n",
        "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((48000, 48, 48, 3), (12000, 48, 48, 3), (48000, 10), (12000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFNfaHTU33eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_WIDTH = 48\n",
        "IMG_HEIGHT = 48\n",
        "IMG_DEPTH = 3\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYcPZT3b33sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = preprocess_input(train_X)\n",
        "valid_X = preprocess_input(valid_X)\n",
        "test_X  = preprocess_input (test_X)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44N5iHAs335w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "outputId": "01cc44d2-d264-45aa-ebc5-a955535166ae"
      },
      "source": [
        "conv_base = keras.applications.vgg16.VGG16(weights='imagenet',\n",
        "                  include_top=False, \n",
        "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
        "                 )\n",
        "conv_base.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 48, 48, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 48, 48, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 6, 6, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXwlSI-u34JT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9aae6210-f6b1-46b0-af59-760dece71182"
      },
      "source": [
        "train_features = conv_base.predict(np.array(train_X), batch_size=BATCH_SIZE, verbose=1)\n",
        "test_features = conv_base.predict(np.array(test_X), batch_size=BATCH_SIZE, verbose=1)\n",
        "val_features = conv_base.predict(np.array(valid_X), batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1675/3000 [===============>..............] - ETA: 48s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG9cdgPf34aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez(\"train_features\", train_features, train_label)\n",
        "np.savez(\"test_features\", test_features, testY)\n",
        "np.savez(\"val_features\", val_features, valid_label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OXN6XVI34pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_features.shape, \"\\n\",  test_features.shape, \"\\n\", val_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0eC9Cmkb8ZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features_flat = np.reshape(train_features, (48000, 1*1*512))\n",
        "#test_features_flat = np.reshape(test_features, (10000, 1*1*512))\n",
        "val_features_flat = np.reshape(val_features, (12000, 1*1*512))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COizpspTb8ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTL8dU1n4HvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_TRAIN_SAMPLES = train_features_flat.shape[0]\n",
        "NB_VALIDATION_SAMPLES = val_features_flat.shape[0]\n",
        "NB_EPOCHS = 100\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\n",
        "model.add(layers.LeakyReLU(alpha=0.1))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFMattCw4H8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizers.Adam(),\n",
        "  # optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "    metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXWge__J4ILi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_features_flat,\n",
        "    train_label,\n",
        "    epochs=2,\n",
        "    validation_data=(val_features_flat, valid_label),\n",
        "  # callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1araMEKw4Icu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.plot(epochs, acc, 'green', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Training and validation loss')\n",
        "plt.plot(epochs, loss, 'green', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey5HwlNTb81h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7KHdGJQb9IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UaHLKDLas_dF"
      },
      "source": [
        "### Task 1.5 Performance comparison\n",
        "\n",
        "\n",
        "Record the test accuracy achieved at different training configurations above. Which method achieved the highest accuracy? Why did it work better for this problem?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ouK5NY-_pLDK"
      },
      "source": [
        "## Task 2 Fast training of deep networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LgoOE2W1pdfN"
      },
      "source": [
        "###### Task 2.1 Train a highly accurate network for CIFAR10\n",
        "\n",
        "\n",
        "In this task, you will train deep neural networks on the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Compared with the datasets that you have worked on so far, CIFAR10 represents a relatively larger multi-class classification problem and presents a great opportunity for you to solve a \"harder\" problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IaD5oqj3lhuI"
      },
      "source": [
        "#### Task 2.1.1 Document the hardware used\n",
        "\n",
        "Before you start, write down your hardware specifications, including \n",
        "\n",
        "- the GPU model, the number of GPUs, and the GPU memory\n",
        "- the CPU model, the number of CPUs, and the CPU clock speed\n",
        "\n",
        "(Hint: you may find commands like `nvidia-smi`, `lscpu` or `psutil` useful.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gAJgmBF91hii"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "adN9Tq-6lyG-"
      },
      "source": [
        "#### Task 2.1.2 Train a \"shallow\" ConvNet\n",
        "\n",
        "Build a ConvNet with fewer than 10 layers. Train the network until it converges. You will use this network as a baseline for the later experiments. \n",
        "\n",
        "- Plot the training and validation history. \n",
        "- Report the testing accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9NTjVIUkmv7S"
      },
      "source": [
        "#### Task 2.1.3 Train a ResNet\n",
        "\n",
        "Train a residual neural network (ResNet) on the CIFAR10 training data and report the test accuracy and the training time.\n",
        "\n",
        "The ResNet is a popular network architecture for image classification. You may find more information about how ResNet works by reading this [paper](https://arxiv.org/abs/1512.03385).\n",
        "\n",
        "\n",
        "*(You may implement a resnet model or use an existing implementation. In either case, you should not use pretrained network weights.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AH6ZBiECzS75"
      },
      "source": [
        "### Task 2.2 Fast training of ResNet\n",
        "\n",
        "\n",
        "In this task, you will experiment with different ways to reduce the time for training your ResNet on CIFAR10. There are different ways to speed up neural network training; below are two ideas. Please select at least one idea to implement. Explain the experiment steps and report the final performance and training time.\n",
        "\n",
        "#### Option 1. Learning rate schedule\n",
        "\n",
        "Use a learning rate schedule for the training. Some popular learning rate schedules include \n",
        "\n",
        "- the Step Decay learning rate (e.g., see [here](https://github.com/kuangliu/pytorch-cifar))\n",
        "- [Cyclical learning rates](https://arxiv.org/abs/1506.01186)\n",
        "- [The exponential learning rate](https://openreview.net/forum?id=rJg8TeSFDH) \n",
        "\n",
        "Also Keras provides [some convenient functions](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) that you can use.\n",
        "\n",
        "\n",
        "#### Option 2. Look ahead optimiser\n",
        "\n",
        "Read [this paper](https://arxiv.org/abs/1907.08610) and implement the Lookahead optimiser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8cKfAOjpn7c"
      },
      "source": [
        "### Task 2.3 Performance comparison\n",
        "\n",
        "\n",
        "Based on the above experiments, which method or which combination of methods result in the best accuracy with the same training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fYdj1bwQT2",
        "colab_type": "text"
      },
      "source": [
        "## Task 3 Design a novel deep neural network model \n",
        "Here, you have to show your critical idea to design a new neural network model. We will evaluate your results based on the novelty of the model and performance of the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UixQS84wQT3",
        "colab_type": "text"
      },
      "source": [
        "### Task 3.1: The key idea to design a novel deep neural networks for CIFAR10\n",
        "\n",
        "\n",
        "In this task, you will design a novel deep neural networks on the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). CIFAR10 represents a relatively larger multi-class classification problem and presents a great opportunity for you to solve a \"harder\" problem. Different from Task 2, in this task you are required to design a novel neural network and optimize the performance in classification. In your answer, you have to clearly present what the key difference between your model and the classic ones, what the benefits in your design model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE4JZr5nwQT4",
        "colab_type": "text"
      },
      "source": [
        "### Task 3.2: The implementation of the novel deep neural networks for CIFAR10\n",
        "\n",
        "\n",
        "\n",
        "In this task, it requires you to write the codes for model implementation and report the performance. In your results, you have to demonstrate the compared performance of your new model and the state-of-the-art models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hUV0wuZ01DNA"
      },
      "source": [
        "---\n",
        "**END OF PART TWO**"
      ]
    }
  ]
}